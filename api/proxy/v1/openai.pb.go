// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.10
// 	protoc        v6.33.1
// source: api/proxy/v1/openai.proto

package v1

import (
	_ "github.com/go-kratos/kratos/v2/errors"
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

type ErrorReason int32

const (
	// 客户端参数错误（包含角色、内容、参数、请求格式等所有输入验证错误）
	ErrorReason_INVALID_ARGUMENT ErrorReason = 0
	// 响应中没有可用的选项
	ErrorReason_NO_CHOICE ErrorReason = 1
	// 上游 API 服务错误（如 OpenAI API 调用失败）
	ErrorReason_UPSTREAM_API_ERROR ErrorReason = 2
)

// Enum value maps for ErrorReason.
var (
	ErrorReason_name = map[int32]string{
		0: "INVALID_ARGUMENT",
		1: "NO_CHOICE",
		2: "UPSTREAM_API_ERROR",
	}
	ErrorReason_value = map[string]int32{
		"INVALID_ARGUMENT":   0,
		"NO_CHOICE":          1,
		"UPSTREAM_API_ERROR": 2,
	}
)

func (x ErrorReason) Enum() *ErrorReason {
	p := new(ErrorReason)
	*p = x
	return p
}

func (x ErrorReason) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (ErrorReason) Descriptor() protoreflect.EnumDescriptor {
	return file_api_proxy_v1_openai_proto_enumTypes[0].Descriptor()
}

func (ErrorReason) Type() protoreflect.EnumType {
	return &file_api_proxy_v1_openai_proto_enumTypes[0]
}

func (x ErrorReason) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use ErrorReason.Descriptor instead.
func (ErrorReason) EnumDescriptor() ([]byte, []int) {
	return file_api_proxy_v1_openai_proto_rawDescGZIP(), []int{0}
}

type ChatCompletionMessageRole int32

const (
	ChatCompletionMessageRole_CHAT_COMPLETION_MESSAGE_ROLE_UNSPECIFIED ChatCompletionMessageRole = 0
	ChatCompletionMessageRole_CHAT_COMPLETION_MESSAGE_ROLE_SYSTEM      ChatCompletionMessageRole = 1
	ChatCompletionMessageRole_CHAT_COMPLETION_MESSAGE_ROLE_USER        ChatCompletionMessageRole = 2
	ChatCompletionMessageRole_CHAT_COMPLETION_MESSAGE_ROLE_ASSISTANT   ChatCompletionMessageRole = 3
)

// Enum value maps for ChatCompletionMessageRole.
var (
	ChatCompletionMessageRole_name = map[int32]string{
		0: "CHAT_COMPLETION_MESSAGE_ROLE_UNSPECIFIED",
		1: "CHAT_COMPLETION_MESSAGE_ROLE_SYSTEM",
		2: "CHAT_COMPLETION_MESSAGE_ROLE_USER",
		3: "CHAT_COMPLETION_MESSAGE_ROLE_ASSISTANT",
	}
	ChatCompletionMessageRole_value = map[string]int32{
		"CHAT_COMPLETION_MESSAGE_ROLE_UNSPECIFIED": 0,
		"CHAT_COMPLETION_MESSAGE_ROLE_SYSTEM":      1,
		"CHAT_COMPLETION_MESSAGE_ROLE_USER":        2,
		"CHAT_COMPLETION_MESSAGE_ROLE_ASSISTANT":   3,
	}
)

func (x ChatCompletionMessageRole) Enum() *ChatCompletionMessageRole {
	p := new(ChatCompletionMessageRole)
	*p = x
	return p
}

func (x ChatCompletionMessageRole) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (ChatCompletionMessageRole) Descriptor() protoreflect.EnumDescriptor {
	return file_api_proxy_v1_openai_proto_enumTypes[1].Descriptor()
}

func (ChatCompletionMessageRole) Type() protoreflect.EnumType {
	return &file_api_proxy_v1_openai_proto_enumTypes[1]
}

func (x ChatCompletionMessageRole) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use ChatCompletionMessageRole.Descriptor instead.
func (ChatCompletionMessageRole) EnumDescriptor() ([]byte, []int) {
	return file_api_proxy_v1_openai_proto_rawDescGZIP(), []int{1}
}

type ChatCompletionMessage struct {
	state         protoimpl.MessageState    `protogen:"open.v1"`
	Role          ChatCompletionMessageRole `protobuf:"varint,1,opt,name=role,proto3,enum=proxy.v1.ChatCompletionMessageRole" json:"role,omitempty"`
	Content       string                    `protobuf:"bytes,2,opt,name=content,proto3" json:"content,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ChatCompletionMessage) Reset() {
	*x = ChatCompletionMessage{}
	mi := &file_api_proxy_v1_openai_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ChatCompletionMessage) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ChatCompletionMessage) ProtoMessage() {}

func (x *ChatCompletionMessage) ProtoReflect() protoreflect.Message {
	mi := &file_api_proxy_v1_openai_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ChatCompletionMessage.ProtoReflect.Descriptor instead.
func (*ChatCompletionMessage) Descriptor() ([]byte, []int) {
	return file_api_proxy_v1_openai_proto_rawDescGZIP(), []int{0}
}

func (x *ChatCompletionMessage) GetRole() ChatCompletionMessageRole {
	if x != nil {
		return x.Role
	}
	return ChatCompletionMessageRole_CHAT_COMPLETION_MESSAGE_ROLE_UNSPECIFIED
}

func (x *ChatCompletionMessage) GetContent() string {
	if x != nil {
		return x.Content
	}
	return ""
}

type ChatCompletionRequest struct {
	state         protoimpl.MessageState   `protogen:"open.v1"`
	Url           string                   `protobuf:"bytes,1,opt,name=url,proto3" json:"url,omitempty"`
	Model         string                   `protobuf:"bytes,2,opt,name=model,proto3" json:"model,omitempty"`
	Token         string                   `protobuf:"bytes,3,opt,name=token,proto3" json:"token,omitempty"`
	Temperature   float32                  `protobuf:"fixed32,4,opt,name=temperature,proto3" json:"temperature,omitempty"`
	TopP          float32                  `protobuf:"fixed32,5,opt,name=top_p,json=topP,proto3" json:"top_p,omitempty"`
	Messages      []*ChatCompletionMessage `protobuf:"bytes,6,rep,name=messages,proto3" json:"messages,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ChatCompletionRequest) Reset() {
	*x = ChatCompletionRequest{}
	mi := &file_api_proxy_v1_openai_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ChatCompletionRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ChatCompletionRequest) ProtoMessage() {}

func (x *ChatCompletionRequest) ProtoReflect() protoreflect.Message {
	mi := &file_api_proxy_v1_openai_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ChatCompletionRequest.ProtoReflect.Descriptor instead.
func (*ChatCompletionRequest) Descriptor() ([]byte, []int) {
	return file_api_proxy_v1_openai_proto_rawDescGZIP(), []int{1}
}

func (x *ChatCompletionRequest) GetUrl() string {
	if x != nil {
		return x.Url
	}
	return ""
}

func (x *ChatCompletionRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *ChatCompletionRequest) GetToken() string {
	if x != nil {
		return x.Token
	}
	return ""
}

func (x *ChatCompletionRequest) GetTemperature() float32 {
	if x != nil {
		return x.Temperature
	}
	return 0
}

func (x *ChatCompletionRequest) GetTopP() float32 {
	if x != nil {
		return x.TopP
	}
	return 0
}

func (x *ChatCompletionRequest) GetMessages() []*ChatCompletionMessage {
	if x != nil {
		return x.Messages
	}
	return nil
}

type ChatCompletionResponse struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Content       string                 `protobuf:"bytes,1,opt,name=content,proto3" json:"content,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ChatCompletionResponse) Reset() {
	*x = ChatCompletionResponse{}
	mi := &file_api_proxy_v1_openai_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ChatCompletionResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ChatCompletionResponse) ProtoMessage() {}

func (x *ChatCompletionResponse) ProtoReflect() protoreflect.Message {
	mi := &file_api_proxy_v1_openai_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ChatCompletionResponse.ProtoReflect.Descriptor instead.
func (*ChatCompletionResponse) Descriptor() ([]byte, []int) {
	return file_api_proxy_v1_openai_proto_rawDescGZIP(), []int{2}
}

func (x *ChatCompletionResponse) GetContent() string {
	if x != nil {
		return x.Content
	}
	return ""
}

type StreamChatCompletionRequest struct {
	state         protoimpl.MessageState   `protogen:"open.v1"`
	Url           string                   `protobuf:"bytes,1,opt,name=url,proto3" json:"url,omitempty"`
	Model         string                   `protobuf:"bytes,2,opt,name=model,proto3" json:"model,omitempty"`
	Token         string                   `protobuf:"bytes,3,opt,name=token,proto3" json:"token,omitempty"`
	Temperature   float32                  `protobuf:"fixed32,4,opt,name=temperature,proto3" json:"temperature,omitempty"`
	TopP          float32                  `protobuf:"fixed32,5,opt,name=top_p,json=topP,proto3" json:"top_p,omitempty"`
	Messages      []*ChatCompletionMessage `protobuf:"bytes,6,rep,name=messages,proto3" json:"messages,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *StreamChatCompletionRequest) Reset() {
	*x = StreamChatCompletionRequest{}
	mi := &file_api_proxy_v1_openai_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StreamChatCompletionRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StreamChatCompletionRequest) ProtoMessage() {}

func (x *StreamChatCompletionRequest) ProtoReflect() protoreflect.Message {
	mi := &file_api_proxy_v1_openai_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StreamChatCompletionRequest.ProtoReflect.Descriptor instead.
func (*StreamChatCompletionRequest) Descriptor() ([]byte, []int) {
	return file_api_proxy_v1_openai_proto_rawDescGZIP(), []int{3}
}

func (x *StreamChatCompletionRequest) GetUrl() string {
	if x != nil {
		return x.Url
	}
	return ""
}

func (x *StreamChatCompletionRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *StreamChatCompletionRequest) GetToken() string {
	if x != nil {
		return x.Token
	}
	return ""
}

func (x *StreamChatCompletionRequest) GetTemperature() float32 {
	if x != nil {
		return x.Temperature
	}
	return 0
}

func (x *StreamChatCompletionRequest) GetTopP() float32 {
	if x != nil {
		return x.TopP
	}
	return 0
}

func (x *StreamChatCompletionRequest) GetMessages() []*ChatCompletionMessage {
	if x != nil {
		return x.Messages
	}
	return nil
}

type StreamChatCompletionResponse struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Chunk         string                 `protobuf:"bytes,1,opt,name=chunk,proto3" json:"chunk,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *StreamChatCompletionResponse) Reset() {
	*x = StreamChatCompletionResponse{}
	mi := &file_api_proxy_v1_openai_proto_msgTypes[4]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StreamChatCompletionResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StreamChatCompletionResponse) ProtoMessage() {}

func (x *StreamChatCompletionResponse) ProtoReflect() protoreflect.Message {
	mi := &file_api_proxy_v1_openai_proto_msgTypes[4]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StreamChatCompletionResponse.ProtoReflect.Descriptor instead.
func (*StreamChatCompletionResponse) Descriptor() ([]byte, []int) {
	return file_api_proxy_v1_openai_proto_rawDescGZIP(), []int{4}
}

func (x *StreamChatCompletionResponse) GetChunk() string {
	if x != nil {
		return x.Chunk
	}
	return ""
}

var File_api_proxy_v1_openai_proto protoreflect.FileDescriptor

const file_api_proxy_v1_openai_proto_rawDesc = "" +
	"\n" +
	"\x19api/proxy/v1/openai.proto\x12\bproxy.v1\x1a\x13errors/errors.proto\"j\n" +
	"\x15ChatCompletionMessage\x127\n" +
	"\x04role\x18\x01 \x01(\x0e2#.proxy.v1.ChatCompletionMessageRoleR\x04role\x12\x18\n" +
	"\acontent\x18\x02 \x01(\tR\acontent\"\xc9\x01\n" +
	"\x15ChatCompletionRequest\x12\x10\n" +
	"\x03url\x18\x01 \x01(\tR\x03url\x12\x14\n" +
	"\x05model\x18\x02 \x01(\tR\x05model\x12\x14\n" +
	"\x05token\x18\x03 \x01(\tR\x05token\x12 \n" +
	"\vtemperature\x18\x04 \x01(\x02R\vtemperature\x12\x13\n" +
	"\x05top_p\x18\x05 \x01(\x02R\x04topP\x12;\n" +
	"\bmessages\x18\x06 \x03(\v2\x1f.proxy.v1.ChatCompletionMessageR\bmessages\"2\n" +
	"\x16ChatCompletionResponse\x12\x18\n" +
	"\acontent\x18\x01 \x01(\tR\acontent\"\xcf\x01\n" +
	"\x1bStreamChatCompletionRequest\x12\x10\n" +
	"\x03url\x18\x01 \x01(\tR\x03url\x12\x14\n" +
	"\x05model\x18\x02 \x01(\tR\x05model\x12\x14\n" +
	"\x05token\x18\x03 \x01(\tR\x05token\x12 \n" +
	"\vtemperature\x18\x04 \x01(\x02R\vtemperature\x12\x13\n" +
	"\x05top_p\x18\x05 \x01(\x02R\x04topP\x12;\n" +
	"\bmessages\x18\x06 \x03(\v2\x1f.proxy.v1.ChatCompletionMessageR\bmessages\"4\n" +
	"\x1cStreamChatCompletionResponse\x12\x14\n" +
	"\x05chunk\x18\x01 \x01(\tR\x05chunk*b\n" +
	"\vErrorReason\x12\x1a\n" +
	"\x10INVALID_ARGUMENT\x10\x00\x1a\x04\xa8E\x90\x03\x12\x13\n" +
	"\tNO_CHOICE\x10\x01\x1a\x04\xa8E\xf4\x03\x12\x1c\n" +
	"\x12UPSTREAM_API_ERROR\x10\x02\x1a\x04\xa8E\xf6\x03\x1a\x04\xa0E\xf4\x03*\xc5\x01\n" +
	"\x19ChatCompletionMessageRole\x12,\n" +
	"(CHAT_COMPLETION_MESSAGE_ROLE_UNSPECIFIED\x10\x00\x12'\n" +
	"#CHAT_COMPLETION_MESSAGE_ROLE_SYSTEM\x10\x01\x12%\n" +
	"!CHAT_COMPLETION_MESSAGE_ROLE_USER\x10\x02\x12*\n" +
	"&CHAT_COMPLETION_MESSAGE_ROLE_ASSISTANT\x10\x032\xca\x01\n" +
	"\x06OpenAI\x12U\n" +
	"\x0eChatCompletion\x12\x1f.proxy.v1.ChatCompletionRequest\x1a .proxy.v1.ChatCompletionResponse\"\x00\x12i\n" +
	"\x14StreamChatCompletion\x12%.proxy.v1.StreamChatCompletionRequest\x1a&.proxy.v1.StreamChatCompletionResponse\"\x000\x01B3Z1github.com/wolodata/proxy-service/api/proxy/v1;v1b\x06proto3"

var (
	file_api_proxy_v1_openai_proto_rawDescOnce sync.Once
	file_api_proxy_v1_openai_proto_rawDescData []byte
)

func file_api_proxy_v1_openai_proto_rawDescGZIP() []byte {
	file_api_proxy_v1_openai_proto_rawDescOnce.Do(func() {
		file_api_proxy_v1_openai_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_api_proxy_v1_openai_proto_rawDesc), len(file_api_proxy_v1_openai_proto_rawDesc)))
	})
	return file_api_proxy_v1_openai_proto_rawDescData
}

var file_api_proxy_v1_openai_proto_enumTypes = make([]protoimpl.EnumInfo, 2)
var file_api_proxy_v1_openai_proto_msgTypes = make([]protoimpl.MessageInfo, 5)
var file_api_proxy_v1_openai_proto_goTypes = []any{
	(ErrorReason)(0),                     // 0: proxy.v1.ErrorReason
	(ChatCompletionMessageRole)(0),       // 1: proxy.v1.ChatCompletionMessageRole
	(*ChatCompletionMessage)(nil),        // 2: proxy.v1.ChatCompletionMessage
	(*ChatCompletionRequest)(nil),        // 3: proxy.v1.ChatCompletionRequest
	(*ChatCompletionResponse)(nil),       // 4: proxy.v1.ChatCompletionResponse
	(*StreamChatCompletionRequest)(nil),  // 5: proxy.v1.StreamChatCompletionRequest
	(*StreamChatCompletionResponse)(nil), // 6: proxy.v1.StreamChatCompletionResponse
}
var file_api_proxy_v1_openai_proto_depIdxs = []int32{
	1, // 0: proxy.v1.ChatCompletionMessage.role:type_name -> proxy.v1.ChatCompletionMessageRole
	2, // 1: proxy.v1.ChatCompletionRequest.messages:type_name -> proxy.v1.ChatCompletionMessage
	2, // 2: proxy.v1.StreamChatCompletionRequest.messages:type_name -> proxy.v1.ChatCompletionMessage
	3, // 3: proxy.v1.OpenAI.ChatCompletion:input_type -> proxy.v1.ChatCompletionRequest
	5, // 4: proxy.v1.OpenAI.StreamChatCompletion:input_type -> proxy.v1.StreamChatCompletionRequest
	4, // 5: proxy.v1.OpenAI.ChatCompletion:output_type -> proxy.v1.ChatCompletionResponse
	6, // 6: proxy.v1.OpenAI.StreamChatCompletion:output_type -> proxy.v1.StreamChatCompletionResponse
	5, // [5:7] is the sub-list for method output_type
	3, // [3:5] is the sub-list for method input_type
	3, // [3:3] is the sub-list for extension type_name
	3, // [3:3] is the sub-list for extension extendee
	0, // [0:3] is the sub-list for field type_name
}

func init() { file_api_proxy_v1_openai_proto_init() }
func file_api_proxy_v1_openai_proto_init() {
	if File_api_proxy_v1_openai_proto != nil {
		return
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_api_proxy_v1_openai_proto_rawDesc), len(file_api_proxy_v1_openai_proto_rawDesc)),
			NumEnums:      2,
			NumMessages:   5,
			NumExtensions: 0,
			NumServices:   1,
		},
		GoTypes:           file_api_proxy_v1_openai_proto_goTypes,
		DependencyIndexes: file_api_proxy_v1_openai_proto_depIdxs,
		EnumInfos:         file_api_proxy_v1_openai_proto_enumTypes,
		MessageInfos:      file_api_proxy_v1_openai_proto_msgTypes,
	}.Build()
	File_api_proxy_v1_openai_proto = out.File
	file_api_proxy_v1_openai_proto_goTypes = nil
	file_api_proxy_v1_openai_proto_depIdxs = nil
}
